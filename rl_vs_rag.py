from typing import Dict, List, Tuple
import json
import gradio as gr

from rag import basic_rag_pipeline, construct_prompt, generate_response, retrieve_relevant_chunks
from rl import calculate_reward, initialize_training_params
from rl_loop import training_loop

# Function to compare Simple RAG vs RL-Enhanced RAG
def compare_rag_approaches(query_text = "When does the park open?", ground_truth = "2031") -> str:
    """
    Compare the outputs of simple RAG versus RL-enhanced RAG.

    Args:
        query_text (str): The input query text for the RAG pipeline.
        ground_truth (str): The expected correct answer for the query.

    Returns:
        Tuple[str, str, float, float]: A tuple containing:
            - simple_response (str): The response generated by the simple RAG pipeline.
            - best_rl_response (str): The best response generated by the RL-enhanced RAG pipeline.
            - simple_similarity (float): The similarity score of the simple RAG response to the ground truth.
            - rl_similarity (float): The similarity score of the RL-enhanced RAG response to the ground truth.
    """
    print("=" * 80)
    print(f"Query: {query_text}")
    print("=" * 80)
    
    # Step 2: Train the RL-enhanced RAG model
    print("\nTraining RL-enhanced RAG model...")
    # Initialize training parameters (e.g., learning rate, number of episodes, discount factor).
    params: Dict[str, float | int] = initialize_training_params()
    # Set the number of episodes to a smaller value for demonstration purposes.
    params["num_episodes"] = 10
    
        # Run the training loop for the RL-enhanced RAG model.
    # This loop trains the model to optimize its responses using reinforcement learning.
    _, simple_response, simple_similarity, rewards_history, actions_history, best_rl_response, best_rl_similarity, improvement  = training_loop(
        query_text, ground_truth, params
    )
    
    print("\nRL-enhanced RAG Output:")
    print("-" * 40)
    print(best_rl_response)
    print(f"Similarity to ground truth: {best_rl_similarity:.4f}")
    
    # Step 3: Evaluate and compare the results
    # Calculate the improvement in similarity score achieved by the RL-enhanced RAG model.
    print(simple_similarity)

    print("\nEvaluation Results:")
    print("-" * 40)
    print(f"Simple RAG similarity to ground truth: {simple_similarity:.4f}")
    print(f"RL-enhanced RAG similarity to ground truth: {best_rl_similarity:.4f}")
    print(f"Improvement: {improvement * 100:.2f}%")
    
    # Step 4: Find the episode with the highest reward
    best_episode_index = rewards_history.index(max(rewards_history))
    best_episode_actions = actions_history[best_episode_index]
    
    # Plot the reward history (if there are enough episodes and matplotlib is available)
    if len(rewards_history) > 1:
        # Return the results: responses and similarity scores for both approaches.
        return json.dumps({
            "simple_response": simple_response,
            "best_rl_response": best_rl_response,
            "simple_similarity": simple_similarity,
            "rl_similarity": best_rl_similarity,
            "improvement": f"{improvement * 100:.2f}%",
            "best_episode_index": best_episode_index + 1,
            "best_episode_actions": best_episode_actions
        }, indent=2)
